{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_uuid": "53e265d56719498b6c971b5cf8ac4c5b727bbd19"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import PIL\n",
    "from PIL import Image\n",
    "from PIL.ImageDraw import Draw\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from scipy.ndimage import affine_transform\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget https://www.kaggleusercontent.com/kf/4346884/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..DyxYyheDh_RA5kd4_MHPwQ.875NZZPJbHPYKhkWxQjO1tIJ01qGMAJ694LuYv3CyQNrCrpFKPRXXjhvOt7-Aa6SxPWVNeo5F1_8K25uGevhDpq4xSvoeoqwNt1t0iMTglPBXCZh5cua_j4eZ2ezs9MnAKzQBeyxDxj8Z6cYpxnat56CkDUIIbjnJZL0o47Afgdk9nmIMZtlgfDJ-OH4XZ1Pv-Eg6wyw9ngGBjIVCjnxCxSHS2lYalz-IsIP5wRIjOxVMrUMDgR_AGgkCIvOgkHpMDXTpoPHJ53czO3zkMQ86GBIQAQcoZcHQzpkdE-gEaeDycqK5B_gH865TtbLK64tw0Rr2_F7u8uikrKgqVHMU8tflZbyzUbWconcEZ24dQ7ojWkuCs2mvdiIyoWLzaPCsE_pMoLAo1i0ytrEWyGHemb9kTnaVDRIIlktevTAefNf0uPzwfQE-U9MwzBcpJtKQ0magKgLg8Xu6mV_YmcU0zFx4Wl51cU9doB-9kh1L9ot6y9sIKAD733cqmiCvhblmK_cwU5hAt7cW28BiaVrtoIkJuAkvrkhvmDIYhHFX4klj9Yv4TAKq0BZ8rmmXuc403MRPLNGqSLYz5Rv-jltXxnQMYIt2kCZmHGklbJU6OUyY2WXhCzZ1yZB1F-mtvd2vwwb_P6HOk8zLCOx5ODy5DMcFA5s5-eb2Qpo9ADpUq8.fJT6QVF86y6-GG6dz0fkzA/cropping.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_uuid": "4a432aea33948e321cf25617d67928097df0c317"
   },
   "outputs": [],
   "source": [
    "MODEL_BASE = '../input/bounding-box-model'\n",
    "DATA = '../WC_input/data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "6a7af444edd754740b2beb1fe32116a3aae6035c"
   },
   "outputs": [],
   "source": [
    "model = load_model('cropping.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, 128, 128, 1) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 128, 128, 64) 5248        input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 128, 128, 64) 36928       conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 128, 128, 64) 256         conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 64, 64, 64)   16448       batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, 64, 64, 64)   36928       conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_22 (Conv2D)              (None, 64, 64, 64)   36928       conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 64, 64, 64)   256         conv2d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_23 (Conv2D)              (None, 32, 32, 64)   16448       batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_24 (Conv2D)              (None, 32, 32, 64)   36928       conv2d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_25 (Conv2D)              (None, 32, 32, 64)   36928       conv2d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 32, 32, 64)   256         conv2d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_26 (Conv2D)              (None, 16, 16, 64)   16448       batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_27 (Conv2D)              (None, 16, 16, 64)   36928       conv2d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_28 (Conv2D)              (None, 16, 16, 64)   36928       conv2d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 16, 16, 64)   256         conv2d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_29 (Conv2D)              (None, 8, 8, 64)     16448       batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_30 (Conv2D)              (None, 8, 8, 64)     36928       conv2d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_31 (Conv2D)              (None, 8, 8, 64)     36928       conv2d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 8, 8, 64)     256         conv2d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_32 (Conv2D)              (None, 4, 4, 64)     16448       batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_33 (Conv2D)              (None, 4, 4, 64)     36928       conv2d_32[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_34 (Conv2D)              (None, 4, 4, 64)     36928       conv2d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 4, 4, 64)     256         conv2d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 4, 1, 64)     0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2D)  (None, 1, 4, 64)     0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)             (None, 256)          0           max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_4 (Flatten)             (None, 256)          0           max_pooling2d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 16)           4112        flatten_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 16)           4112        flatten_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 32)           0           dense_4[0][0]                    \n",
      "                                                                 dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 4)            132         concatenate_2[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 503,588\n",
      "Trainable params: 502,820\n",
      "Non-trainable params: 768\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "3b03b5b50d2c4787d9d6c065a160316ddecd45fc"
   },
   "outputs": [],
   "source": [
    "data_paths = [img for img in os.listdir(DATA)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_uuid": "0b45eea2cd93933ea852d08a30c846440eeb6376"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Define useful constants\n",
    "img_shape  = (384, 192, 1)\n",
    "anisotropy = 2.15\n",
    "\n",
    "def center_transform(affine, input_shape):\n",
    "    hi, wi = float(input_shape[0]), float(input_shape[1])\n",
    "    ho, wo = float(img_shape[0]), float(img_shape[1])\n",
    "    top, left, bottom, right = 0, 0, hi, wi\n",
    "    if wi/hi/anisotropy < wo/ho: # input image too narrow, extend width\n",
    "        w     = hi*wo/ho*anisotropy\n",
    "        left  = (wi-w)/2\n",
    "        right = left + w\n",
    "    else: # input image too wide, extend height\n",
    "        h      = wi*ho/wo/anisotropy\n",
    "        top    = (hi-h)/2\n",
    "        bottom = top + h\n",
    "    center_matrix   = np.array([[1, 0, -ho/2], [0, 1, -wo/2], [0, 0, 1]])\n",
    "    scale_matrix    = np.array([[(bottom - top)/ho, 0, 0], [0, (right - left)/wo, 0], [0, 0, 1]])\n",
    "    decenter_matrix = np.array([[1, 0, hi/2], [0, 1, wi/2], [0, 0, 1]])\n",
    "    return np.dot(np.dot(decenter_matrix, scale_matrix), np.dot(affine, center_matrix))\n",
    "\n",
    "# Apply an affine transformation to an image represented as a numpy array.\n",
    "def transform_img(x, affine):\n",
    "    matrix   = affine[:2,:2]\n",
    "    offset   = affine[:2,2]\n",
    "    x        = np.moveaxis(x, -1, 0)\n",
    "    channels = [affine_transform(channel, matrix, offset, output_shape=img_shape[:-1], order=1,\n",
    "                                 mode='constant', cval=np.average(channel)) for channel in x]\n",
    "    return np.moveaxis(np.stack(channels, axis=0), 0, -1)\n",
    "\n",
    "def read_raw_image(p):\n",
    "    return Image.open(p)\n",
    "\n",
    "def read_for_validation(x):\n",
    "    t  = np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1]])\n",
    "    t  = center_transform(t, x.shape)\n",
    "    x  = transform_img(x, t)\n",
    "    x -= np.mean(x, keepdims=True)\n",
    "    x /= np.std(x, keepdims=True) + K.epsilon()\n",
    "    return x, t\n",
    "\n",
    "def coord_transform(list, trans):\n",
    "    result = []\n",
    "    for x,y in list:\n",
    "        y,x,_ = trans.dot([y,x,1]).astype(np.int)\n",
    "        result.append((x,y))\n",
    "    return result\n",
    "\n",
    "def read_array(p):\n",
    "    img = read_raw_image(p).convert('L')\n",
    "    return img_to_array(img)\n",
    "\n",
    "def make_bbox(p):\n",
    "    raw = read_array(p)\n",
    "    width, height = raw.shape[1], raw.shape[0]\n",
    "    img,trans         = read_for_validation(raw)\n",
    "    a                 = np.expand_dims(img, axis=0)\n",
    "    x0, y0, x1, y1    = model.predict(a).squeeze()\n",
    "    (u0, v0),(u1, v1) = coord_transform([(x0,y0),(x1,y1)], trans)\n",
    "    bbox = [max(u0,0), max(v0,0), min(u1,width), min(v1,height)]\n",
    "    if bbox[0] >= bbox[2] or bbox[1] >= bbox[3]:\n",
    "        bbox = [0,0,width,height]\n",
    "    return bbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_uuid": "6bbddf36e5cbc397e7baa1b87635063f6da8cbef"
   },
   "outputs": [],
   "source": [
    "bbox_df = pd.DataFrame(columns=['Image','x0','y0','x1','y1']).set_index('Image')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_uuid": "36085d38dc2767f034317bfb05e587d386e137dd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5347 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected input_2 to have shape (128, 128, 1) but got array with shape (384, 192, 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-307102c87660>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_paths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mbbox_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_bbox\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATA\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-7-b6866da75c16>\u001b[0m in \u001b[0;36mmake_bbox\u001b[0;34m(p)\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrans\u001b[0m         \u001b[0;34m=\u001b[0m \u001b[0mread_for_validation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0ma\u001b[0m                 \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m     \u001b[0mx0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my1\u001b[0m    \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m     \u001b[0;34m(\u001b[0m\u001b[0mu0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mu1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcoord_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrans\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0mbbox\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mu0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mu1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mwidth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mheight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/kerasgpu/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1011\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1012\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1013\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m   1014\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1015\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/kerasgpu/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, model, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    496\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPREDICT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    497\u001b[0m         \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 498\u001b[0;31m         workers=workers, use_multiprocessing=use_multiprocessing, **kwargs)\n\u001b[0m\u001b[1;32m    499\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    500\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/kerasgpu/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36m_model_iteration\u001b[0;34m(self, model, mode, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    424\u001b[0m           \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m           \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 426\u001b[0;31m           use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    427\u001b[0m       \u001b[0mtotal_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_total_number_of_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madapter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m       \u001b[0muse_sample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtotal_samples\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/kerasgpu/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36m_process_inputs\u001b[0;34m(model, mode, x, y, batch_size, epochs, sample_weights, class_weights, shuffle, steps, distribution_strategy, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    644\u001b[0m     \u001b[0mstandardize_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    645\u001b[0m     x, y, sample_weights = standardize(\n\u001b[0;32m--> 646\u001b[0;31m         x, y, sample_weight=sample_weights)\n\u001b[0m\u001b[1;32m    647\u001b[0m   \u001b[0;32melif\u001b[0m \u001b[0madapter_cls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mdata_adapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mListsOfScalarsDataAdapter\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    648\u001b[0m     \u001b[0mstandardize_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstandardize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/kerasgpu/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\u001b[0m\n\u001b[1;32m   2381\u001b[0m         \u001b[0mis_dataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2382\u001b[0m         \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2383\u001b[0;31m         batch_size=batch_size)\n\u001b[0m\u001b[1;32m   2384\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2385\u001b[0m   def _standardize_tensors(self, x, y, sample_weight, run_eagerly, dict_inputs,\n",
      "\u001b[0;32m/anaconda/envs/kerasgpu/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_tensors\u001b[0;34m(self, x, y, sample_weight, run_eagerly, dict_inputs, is_dataset, class_weight, batch_size)\u001b[0m\n\u001b[1;32m   2408\u001b[0m           \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2409\u001b[0m           \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2410\u001b[0;31m           exception_prefix='input')\n\u001b[0m\u001b[1;32m   2411\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2412\u001b[0m     \u001b[0;31m# Get typespecs for the input data and sanitize it if necessary.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/kerasgpu/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    580\u001b[0m                              \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m                              \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 582\u001b[0;31m                              str(data_shape))\n\u001b[0m\u001b[1;32m    583\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    584\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking input: expected input_2 to have shape (128, 128, 1) but got array with shape (384, 192, 1)"
     ]
    }
   ],
   "source": [
    "for img in tqdm(data_paths):\n",
    "    bbox_df.loc[img] = make_bbox(os.path.join(DATA,img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d9b9cfb37ddb3f0699a1fb37862b988bead01a1a"
   },
   "outputs": [],
   "source": [
    "bbox_df.to_csv(\"bboxs.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(bbox_df.y1).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:kerasgpu]",
   "language": "python",
   "name": "conda-env-kerasgpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
